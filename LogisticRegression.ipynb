{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPIOMhYTY9VVVkBAMQUZkRY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Logistic regression is a specific type pf generalized linear model (GLM) which are a generalization of the concepts and abilities of linear models\n","\n","Logistic regression is similar to linear regression except the while the linear regression is not contricted to a domain it can theoretically take any value, however logistic regression is contrained between 0 and 1 and to solve this, we transform the y axis from \"the probability of class\" to the \"log(odd of class)\" so, just like the linear regression it's y-axis can go from -infinity to +infinity\n","\n","Now to fit a line in logistic regression we first take a candidate line then project the original data onto the candidate line, this gives each sample a candidate log(odds) value. Then we transform the candidate log(odds) to candidate probabilities. We calculate the positive class likelihood of the line then we rotate the candidate line on the log(odds) graph convert it to probabilities and calculate it's likelihood. We repeat this until we get a line with the highest likelihood value.\n","\n","Let us try an build a Logistic regression model from scratch"],"metadata":{"id":"V3FUVd1zyHVU"}},{"cell_type":"code","source":["import numpy as np\n","\n","\n","class CustomLogisticRegression:\n","  def __init__(self, lr=0.001, n_iters=1000):\n","    self.lr = lr\n","    self.n_iters = n_iters\n","    self.weights = None\n","    self.bias = None\n","\n","  def fit(self, X, y):\n","    n_samples, n_features = X.shape\n","    self.weights = np.zeros(n_features)\n","    self.bias = 0\n","\n","    for _ in range(self.n_iters):\n","      predictions = self._predict(X)\n","\n","      dw = (1/n_samples) * np.dot(X.T, (predictions - y))\n","      db = (1/n_samples) * np.sum(predictions - y)\n","\n","      self.weights -= self.lr * dw\n","      self.bias -= self.lr * db\n","\n","  def predict(self, X):\n","    y_pred = self._sigmoid(np.dot(X, self.weights) + self.bias)\n","    y_pred_cls = [1 if i > 0.5 else 0 for i in y_pred]\n","    return y_pred_cls\n","\n","  def _predict(self, X):\n","    return self._sigmoid(np.dot(X, self.weights) + self.bias)\n","\n","  def _sigmoid(self, x):\n","    return 1/(1 + np.exp(-x))"],"metadata":{"id":"sXzh0_aaO8mR","executionInfo":{"status":"ok","timestamp":1740071314699,"user_tz":-180,"elapsed":8,"user":{"displayName":"Adil Ali","userId":"00301285379239721154"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from sklearn.linear_model import LogisticRegression\n","\n","bc = datasets.load_breast_cancer()\n","X, y = bc.data, bc.target\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","clf = CustomLogisticRegression(lr=0.01)\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","\n","sk_clf = LogisticRegression()\n","sk_clf.fit(X_train, y_train)\n","sk_y_pred = sk_clf.predict(X_test)\n","\n","def accuracy(y_pred, y_test):\n","  return np.sum(y_pred == y_test) / len(y_test)\n","\n","acc = accuracy(y_pred, y_test)\n","print(f'custom Logistic Regression accuracy: {acc}')\n","\n","acc = accuracy(sk_y_pred, y_test)\n","print(f'sklearn Logistic Regression accuracy: {acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VIgUrblbT5Ii","executionInfo":{"status":"ok","timestamp":1740071366245,"user_tz":-180,"elapsed":135,"user":{"displayName":"Adil Ali","userId":"00301285379239721154"}},"outputId":"89741f9b-c6c4-4119-a983-8146c03e6e02"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["custom Logistic Regression accuracy: 0.9298245614035088\n","sklearn Logistic Regression accuracy: 0.9385964912280702\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-8-4b83dd48a5db>:34: RuntimeWarning: overflow encountered in exp\n","  return 1/(1 + np.exp(-x))\n","/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]}]}